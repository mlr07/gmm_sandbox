{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## GMM from scratch\n",
    "\n",
    "Build a simple implementation of GMM using the expectation-maximization algorithm and numpy operations. Run model on built in IRIS dataset. \n",
    "\n",
    "Based on the article: https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95\n",
    "\n",
    "### Concepts\n",
    "\n",
    "1. Covariance matrix: https://en.wikipedia.org/wiki/Covariance_matrix \n",
    "2. Guassian distribution: https://en.wikipedia.org/wiki/Gaussian_function\n",
    "3. Determinant: https://en.wikipedia.org/wiki/Determinant\n",
    "4. Eigenvectors: https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors\n",
    "\n",
    "### GMM\n",
    "\n",
    "1. User Guide: https://scikit-learn.org/stable/modules/mixture.html\n",
    "2. Class API: https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\n",
    "3. Model selection: https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html\n",
    "4. Expectation-maximization: https://en.wikipedia.org/wiki/Expectation\n",
    "\n",
    "### Parameters\n",
    "\n",
    "1. log-likelihood = probability of parameters for known x outcome\n",
    "2. $\\gamma(z_{nk})$ = expectation probability\n",
    "3. $\\pi_k$ = gaussian mixing component \n",
    "4. $\\mu_k$ = component mean\n",
    "5. $\\Sigma_k$ = component covariance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "# build dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move to module\n",
    "\n",
    "def gdf(X, mu, cov):\n",
    "    # get number of elements in instance \n",
    "    n = X.shape[1]\n",
    "    # substract features from mean, transpose for eventual dot product\n",
    "    diff = (X - mu).T\n",
    "    # compute left side, return a single float\n",
    "    left = 1/((2*np.pi)**(n/2)*np.linalg.det(cov)**0.5)\n",
    "    # compute right side, extract values on diagnonal of matrix, return row vector\n",
    "    right = np.diagonal(np.exp(-0.5*np.dot(np.dot(diff.T, np.linalg.inv(cov)), diff)))\n",
    "    # multiply left scaler by right vector, reshape to column vector and return\n",
    "    gdf = (left*right).reshape(-1,1)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def init_clusters(X, n_clusters):\n",
    "    clusters = []\n",
    "    # fit iris data to kmeans\n",
    "    kmeans = KMeans().fit(X)\n",
    "    # extract 4 cluster means\n",
    "    mu_k = kmeans.cluster_centers_\n",
    "    \n",
    "    # build dict of init params for each cluster \n",
    "    for i in range(n_clusters):\n",
    "        # append param key value pairs to list\n",
    "        clusters.append({\n",
    "            # inverse of n_cluster\n",
    "            'pi_k': 1.0 / n_clusters,\n",
    "            # means for a given cluster\n",
    "            'mu_k': mu_k[i],\n",
    "            # n x n identity matrix from dimensions of iris data\n",
    "            'cov_k': np.identity(X.shape[1], dtype=np.float64)\n",
    "        })\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "\n",
    "def expectation_step(X, clusters):\n",
    "    # make array to hold totals\n",
    "    totals = np.zeros((X.shape[0], 1), dtype=np.float64)\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        # grab param values\n",
    "        pi_k = cluster['pi_k']\n",
    "        mu_k = cluster['mu_k']\n",
    "        cov_k = cluster['cov_k']\n",
    "        # compute numerator of gamma and return a column vector of length X\n",
    "        gamma_nk = (pi_k * gaussian(X, mu_k, cov_k)).astype(np.float64)\n",
    "\n",
    "        # compute denominator of gamma as sum of numerator terms and return a column vector of length X\n",
    "        for i in range(X.shape[0]):\n",
    "            totals[i] += gamma_nk[i]\n",
    "        \n",
    "        # assign to keys in dictionary\n",
    "        cluster['gamma_nk'] = gamma_nk\n",
    "        cluster['totals'] = totals\n",
    "    \n",
    "    # complete computation of gamma_nk, not the biggest fan of this method\n",
    "    for cluster in clusters:\n",
    "        cluster['gamma_nk'] /= cluster['totals']\n",
    "\n",
    "\n",
    "def maximization_step(X, clusters):\n",
    "    # get N equal to shape of X\n",
    "    N = float(X.shape[0])\n",
    "  \n",
    "    for cluster in clusters:\n",
    "        # grab gamma column vector\n",
    "        gamma_nk = cluster['gamma_nk']\n",
    "        # make n x n cov matrix from iris dimensions \n",
    "        cov_k = np.zeros((X.shape[1], X.shape[1]))\n",
    "        # get sum of gamma\n",
    "        N_k = np.sum(gamma_nk, axis=0)\n",
    "        # compute updated pi_k\n",
    "        pi_k = N_k / N\n",
    "        # compute updated mu_k\n",
    "        mu_k = np.sum(gamma_nk * X, axis=0) / N_k\n",
    "        \n",
    "        # compute updated cov_k\n",
    "        for j in range(X.shape[0]):\n",
    "            diff = (X[j] - mu_k).reshape(-1, 1)\n",
    "            cov_k += gamma_nk[j] * np.dot(diff, diff.T)\n",
    "        \n",
    "        # complete cov_k\n",
    "        cov_k /= N_k\n",
    "        \n",
    "        # update parameters in dict\n",
    "        cluster['pi_k'] = pi_k\n",
    "        cluster['mu_k'] = mu_k\n",
    "        cluster['cov_k'] = cov_k\n",
    "\n",
    "\n",
    "def get_likelihood(X, clusters):\n",
    "    # compute sample likelihoods by accessing totals key in each cluster\n",
    "    return np.log(np.array([cluster['totals'] for cluster in clusters]))\n",
    "\n",
    "\n",
    "# nest init, expectation, and maximization in a training function\n",
    "# cluster list of dictionaries are avaialbe in local scope of function\n",
    "def train_gmm(X, n_clusters, n_epochs):\n",
    "    # init clusters \n",
    "    clusters = init_clusters(X, n_clusters)\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # run expectation over each epoch\n",
    "        expectation_step(X, clusters)\n",
    "        # run maximization over each epoch\n",
    "        maximization_step(X, clusters)\n",
    "        # compute likelihoods \n",
    "        sample_likelihoods = get_likelihood(X, clusters)\n",
    "        \n",
    "    return clusters, sample_likelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clusters to search for and epochs to run EM over\n",
    "n_clusters = 3\n",
    "n_epochs = 50\n",
    "\n",
    "# run scratch routine and assign values to variables\n",
    "clusters, sample_likelihoods = train_gmm(X, n_clusters, n_epochs)\n",
    "\n",
    "# construct and fit sklearn gmm to iris data\n",
    "gmm = GaussianMixture(n_components=n_clusters, max_iter=n_epochs).fit(X)\n",
    "# grab model log-lilelihood\n",
    "gmm_scores = gmm.score_samples(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Means by sklearn:\n [[5.006      3.428      1.462      0.246     ]\n [5.91697517 2.77803998 4.20523542 1.29841561]\n [6.54632887 2.94943079 5.4834877  1.98716063]]\nMeans by our implementation:\n [[5.91496959 2.77784365 4.20155323 1.29696685]\n [5.006      3.428      1.462      0.246     ]\n [6.54454865 2.94866115 5.47955343 1.98460495]]\nScores by sklearn:\n [ 1.57050082  0.73787138  1.14436656  0.92913238  1.411028   -0.09451903\n  0.05266884  1.62442195  0.27082378  0.16706624  0.83489877  0.77168582\n  0.29597841 -1.79224582 -3.41557928 -2.10529279 -1.12995447  1.47503579\n -0.84612536  0.97699215]\nScores by our implementation:\n [ 1.57057947  0.73793642  1.14444614  0.92920539  1.41110417 -0.09448868\n  0.05268031  1.62449505  0.27090462  0.16702226  0.83494742  0.77171947\n  0.29597776 -1.79222469 -3.41562626 -2.1052825  -1.1300608   1.47509939\n -0.84608424  0.9770596 ]\n"
     ]
    }
   ],
   "source": [
    "# comparison between scratch gmm and sklearn gmm\n",
    "\n",
    "# cluster means for scratch and sklearn\n",
    "print('Means by sklearn:\\n', gmm.means_)\n",
    "print('Means by our implementation:\\n', np.array([cluster['mu_k'].tolist() for cluster in clusters]))\n",
    "\n",
    "# model likelihood scores for scratch and sklearn\n",
    "print('Scores by sklearn:\\n', gmm_scores[0:20])\n",
    "print('Scores by our implementation:\\n', sample_likelihoods.reshape(-1)[0:20])"
   ]
  }
 ]
}